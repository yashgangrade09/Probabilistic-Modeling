\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Assignment\_1},
            pdfauthor={Yash Gangrade},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{{#1}}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{{#1}}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\ImportTok}[1]{{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{{#1}}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{{#1}}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{{#1}}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{{#1}}}}
\newcommand{\BuiltInTok}[1]{{#1}}
\newcommand{\ExtensionTok}[1]{{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{{#1}}}
\newcommand{\RegionMarkerTok}[1]{{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{{#1}}}}
\newcommand{\NormalTok}[1]{{#1}}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}
  \title{Assignment\_1}
  \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
  \author{Yash Gangrade}
  \preauthor{\centering\large\emph}
  \postauthor{\par}
  \predate{\centering\large\emph}
  \postdate{\par}
  \date{February 8, 2018}


\begin{document}
\maketitle

\section{Written Part}\label{written-part}

\subsection{Question 1}\label{question-1}

\subsubsection{a)}\label{a}

Solution: We have, \[p(x) = h(x)exp(\eta\cdot T(x) - A(\eta))\] Now, we
will use the general identity here:

\begin{align*}
    \int p(x) dx &= 1 \\
    \int h(x)exp(\eta\cdot T(x) - A(\eta)) dx &= 1 \\
    \int h(x)exp(\eta \cdot T(x)) &= exp(A(\eta))
\end{align*}

Now, we will do the partial derivative of the above function w.r.t
\(\eta\), so we have

\begin{align*}
   \frac{\partial \int h(x)exp(\eta \cdot T(x)) dx}{\partial \eta}  &= \frac{\partial exp(A(\eta))}{\partial \eta} \\
   \int h(x) \frac{\partial exp(\eta \cdot T(x))}{\partial \eta}dx  &= \frac{\partial exp(A(\eta))}{\partial \eta} \\
   \int h(x) T(x) exp(\eta \cdot T(x))dx  &= exp(A(\eta))\frac{\partial A(\eta)}{\partial \eta} \\
   \frac{\int h(x) T(x) exp(\eta \cdot T(x))dx}{exp(A(\eta))} &= \frac{\partial A(\eta)}{\partial \eta} \\
   \int h(x) T(x) exp(\eta \cdot T(x) - A(\eta))dx &= \frac{\partial A(\eta)}{\partial \eta} \\
   \int T(x)h(x)exp(\eta \cdot T(x) - A(\eta))dx &= \frac{\partial A(\eta)}{\partial \eta}
\end{align*}

From initial equation and the knowledge of expectations i.e.
\(E[g(x)|y] = \int g(x) p(x|y) dx\). So, here we have,

\begin{align*}
   \int T(x)h(x)exp(\eta \cdot T(x) - A(\eta))dx &= \frac{\partial A(\eta)}{\partial \eta} \\
   \int T(x)p(x|\eta) dx &= \frac{\partial A(\eta)}{\partial \eta} \\
   E[T(x)|\eta] &= \nabla A(\eta) = \left(\frac{\partial A}{\partial \eta_1}, \frac{\partial A}{\partial \eta_2}, ....., \frac{\partial A}{\partial \eta_d}\right)
\end{align*}

Hence proved.

\subsubsection{b)}\label{b}

Solution: Now, we need to verify the above result. Here, we are given a
Gaussian with known variance but unknown mean so we have,
\[X | \mu \sim N(\mu, \sigma^2)\] So, we have the conditional
probability distribution and we need to write it in form of an
exponential family
first.\[p(x|\mu) = \frac{1}{\sqrt{2\pi \sigma^2}} exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right)\]
We have after expanding,

\begin{align*}
    p(x|\mu) &= \frac{1}{\sqrt{2\pi} \sigma} exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right) \\
    p(x|\mu) &= \frac{1}{\sqrt{2\pi} \sigma} exp\left(-\frac{1}{2\sigma^2}(x^2 + \mu^2 - 2x\mu)\right) \\
    p(x|\mu) &= \frac{1}{\sqrt{2\pi} \sigma} exp\left(-\frac{1}{2\sigma^2}\right)exp\left(-\frac{\mu^2}{2\sigma^2} + \frac{x\mu}{\sigma^2}\right) \\
\end{align*}

Comparing the above equation with the standard exponential family
function \(p(x) = h(x)exp(\eta\cdot T(x) - A(\eta))\), we have,

\begin{align*}
    \eta &= \mu \\
    h(x) &= \frac{1}{\sqrt{2\pi} \sigma}exp\left(-\frac{1}{2\sigma^2}\right) \\
    T(x) &= \frac{x}{\sigma^2} \\
    A(\mu) &= \frac{\mu^2}{2\sigma^2} 
\end{align*}

So, now we have,

\begin{equation}
    E[T(x|\eta)] = \frac{E[x]}{\sigma^2} = \frac{\mu}{\sigma^2}
\end{equation}

And, we have,

\begin{equation}
    \frac{\partial A}{\partial \mu} = \frac{2\mu}{2\sigma^2} = \frac{\mu}{\sigma^2}
\end{equation}

So, from the above two equations, we have verified the result in the
first part.

\subsection{Question 2}\label{question-2}

\subsubsection{a)}\label{a-1}

Solution:

We have, \(X \sim Pois (\lambda)\) and the pmf as:
\[P(X = x; \lambda) = \frac{\lambda^x e^{-\lambda}}{x!}\] Now, we need
to get the exponential family form so, we do,

\begin{align}
    P(X = x; \lambda) &= \frac{\lambda^x e^{-\lambda}}{x!} \\
    P(X = x; \lambda) &= \frac{e^{x\ln \lambda} e^{-\lambda}}{x!} \\
    P(X = x; \lambda) &= \frac{exp(x\ln \lambda -\lambda)}{x!} \\
\end{align}

Comparing with the general form of exponential family form we have,

\begin{align*}
    \eta(\lambda) &= \ln \lambda \qquad \textit{Natural Parameter}\\
    h(x) &= \frac{1}{x!}\\
    T(x) &= x \qquad \textit{Sufficient Statistic}\\
    A(\lambda) &= \lambda 
\end{align*}

\subsubsection{b) and c)}\label{b-and-c}

Solution: Two non-informative priors for this distribution are:
\textbackslash{} \textbf{Jeffrey's Prior} We have the pmf of the
gaussian as, \[P(X = x; \lambda) = \frac{\lambda^x e^{-\lambda}}{x!}\]
Now, we will find the fischer's information i.e. \(I(\lambda)\) for the
jeffrey's prior as follows:

\begin{align*}
    I(\lambda) &= E\left[-\frac{\partial^2 \ln p(x|\lambda)}{\partial \lambda^2}\right] \\
    I(\lambda) &= E\left[-\frac{\partial^2}{\partial \lambda^2} (x\ln \lambda - \lambda - \ln x!)\right] \\
    I(\lambda) &= E\left[-\frac{\partial}{\partial \lambda} \left(\frac{x}{\lambda} - 1\right)\right] \\
    I(\lambda) &= E\left[\frac{x}{\lambda^2}\right] \\
    \text{We know that for a poisson, } & E[x] = \lambda \\
    I(\lambda) &= \frac{1}{\lambda}
\end{align*}

Now, we know that jeffrey's prior is just dependent on the fischer's
information and it is calculated as follows:

\begin{align*}
    P(\lambda) &= \sqrt{I(\lambda)} \\
    P(\lambda) &= \sqrt{\frac{1}{\lambda}} \qquad \forall x \in (0,\infty)
\end{align*}

Following the definition from one of the blogs online
(\url{http://lesswrong.com/lw/6uk/against/_improper/_priors/}), this
prior is an improper prior. Now, we need to calculate the posterior
distribution for this prior. The calculations are as follows:

\begin{align*}
    p(\lambda | x_1, x_2,...., x_n) &= \frac{p(\lambda) p(X|\lambda)}{\sum p(\lambda) p(X|\lambda)} \\
    p(\lambda | x_1, x_2,...., x_n) &= \frac{\frac{1}{\sqrt{\lambda}} \prod_{i=1} \frac{\lambda^{x_i}e^{-\lambda}}{x_i!}}{\sum_{\lambda}\prod_{i=1} \frac{1}{\sqrt{\lambda} }\frac{\lambda^{x_i}e^{-\lambda}}{x_i!}} \\
    p(\lambda | x_1, x_2,...., x_n) &= \frac{\frac{\lambda^{\sum x_i - \frac{1}{2}}\cdot e^{-n\lambda}}{x_1!x_2!....x_n!}}{\sum_{\lambda} \frac{\lambda^{\sum x_i - \frac{1}{2}}\cdot e^{-n\lambda}}{x_1!x_2!....x_n!}} \\
    p(\lambda | x_1, x_2,...., x_n) &= \frac{\lambda^{\sum x_i + \frac{1}{2} - 1}\cdot e^{-n\lambda}}{\sum_{\lambda} \lambda^{\sum x_i + \frac{1}{2} - 1}\cdot e^{-n\lambda}} \\
\end{align*}

The formula above looks similar to the pdf of a Gamma Distribution
\textbackslash{}
(\url{https://en.wikipedia.org/wiki/Gamma/_distribution}), thus we can
define proportionality to it with the factors:
\[p(\lambda | x_1, x_2,...., x_n) \propto Gamma\left(\frac{1}{2} + \sum x_i, n\right)\]

So one interesting thing to observe here is that even though our prior
was improper, the posterior is a proper distribution.

\textbf{Uniform Prior}

We have the pmf of the gaussian as,
\[P(X = x; \lambda) = \frac{\lambda^x e^{-\lambda}}{x!}\] Consider a
uniform prior as it is the max entropy non-informative prior as the
prior. Therefore, we have,

Following the procedure similar to jeffrey's prior, we will do the
calculation as;

\begin{align*}
    p(\lambda | x_1, x_2,...., x_n) &= \frac{p(\lambda) p(X|\lambda)}{\sum p(\lambda) p(X|\lambda)} \\
    p(\lambda | x_1, x_2,...., x_n) &= \frac{1 \cdot \prod_{i=1} \frac{\lambda^{x_i}e^{-\lambda}}{x_i!}}{\sum_{\lambda}\prod_{i=1} \frac{1}{\sqrt{\lambda} }\frac{\lambda^{x_i}e^{-\lambda}}{x_i!}} \\
    p(\lambda | x_1, x_2,...., x_n) &= \frac{\frac{\lambda^{\sum x_i}\cdot e^{-n\lambda}}{x_1!x_2!....x_n!}}{\sum_{\lambda} \frac{\lambda^{\sum x_i}\cdot e^{-n\lambda}}{x_1!x_2!....x_n!}} \\
    p(\lambda | x_1, x_2,...., x_n) &= \frac{\lambda^{\sum x_i + 1 - 1}\cdot e^{-n\lambda}}{\sum_{\lambda} \lambda^{\sum x_i + 1 - 1}\cdot e^{-n\lambda}} \\
\end{align*}

The formula above looks similar to the pdf of a Gamma Distribution
\textbackslash{}
(\url{https://en.wikipedia.org/wiki/Gamma/_distribution}), thus we can
define proportionality to it with the factors:
\[p(\lambda | x_1, x_2,...., x_n) \propto Gamma\left(1 + \sum x_i, n\right)\]

So one interesting thing to observe here is that even though our prior
was improper, the posterior is a proper distribution.

\subsection{Question 3}\label{question-3}

Solution: Given, we have that the \(X_i\) is from a Gaussian
distribution with a known variance \(\sigma^2\) and an unknown \(\mu\)
with a uniform prior. Essentially, we have,

\begin{align*}
    \mu &\sim Unif(a,b) \\
    p(\mu) &= \frac{1}{b-a} \qquad \forall \mu \in [a,b]
\end{align*}

Similarly, we have,

\begin{align*}
    X_i &\sim N(\mu, \sigma^2) \\
    p(x_i|\mu) &= \frac{1}{\sqrt{2\pi}\sigma} exp\left(-\frac{(x_i - \mu)^2}{2\sigma^2}\right) \\
    \text{For all the n variables} & \\
    p(x_1,x_2,..., x_n | \mu) &= \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi}\sigma} exp\left(-\frac{(x_i - \mu)^2}{2\sigma^2}\right) \\
    p(x_1,x_2,..., x_n | \mu) &= \left(\frac{1}{\sqrt{2\pi}\sigma}\right)^n exp\left(-\frac{\sum (x_i - \mu)^2}{2\sigma^2}\right) 
\end{align*}

Now, we need to find the posterior pdf, we have,

\begin{align*}
    p(\mu | x_1, x_2, ....., x_n; \sigma^2, a,b) &= \frac{p(x_1, x_2, ....., x_n | \mu; \sigma^2, a,b)\cdot p(\mu)}{\int_{a}^{b} p(x_1, x_2, ....., x_n | \mu; \sigma^2, a,b)\cdot p(\mu) d\mu} \\
    p(\mu | x_1, x_2, ....., x_n; \sigma^2, a,b) &= \frac{\left(\frac{1}{\sqrt{2\pi}\sigma}\right)^n exp\left(-\frac{\sum (x_i - \mu)^2}{2\sigma^2}\right) \cdot \frac{1}{b-a}}{\int_{a}^{b} \left(\frac{1}{\sqrt{2\pi}\sigma}\right)^n exp\left(-\frac{\sum (x_i - \mu)^2}{2\sigma^2}\right) \cdot \frac{1}{b-a}d\mu}\\
    p(\mu | x_1, x_2, ....., x_n; \sigma^2, a,b) &= \frac{exp\left(-\frac{\sum (x_i - \mu)^2}{2\sigma^2}\right)}{\int_{a}^{b} exp\left(-\frac{\sum (x_i - \mu)^2}{2\sigma^2}\right) d\mu}
\end{align*}

Thus, we can see that the posterior pdf is also a Gaussian Distribution.

\section{R Coding}\label{r-coding}

\subsection{Question 4}\label{question-4}

\subsubsection{a)}\label{a-2}

Solution: For the given distributions we have the joint posterior
density function as:
\[p(\mu_j, \sigma_j^2 | y_{i,j}) \sim N-IG(\mu_n, n_n, \alpha_n, \beta_n)\]
Here, for each \(j\) in range from 1 to 3, we have the number of
elements as \(n_1, ... n_j\), we have for a particular \((i, j)\) pair,
\[
\begin{align*}
    \bar{y} &= \frac{\sum_{i=1}^{n} y_i}{n} \\    
  \mu_n &= \frac{\mu_0 n_0 + n \bar{y}}{n_0 + n} \\
  n_n &= n_0 + n \\
  \alpha_n &= \alpha_0 + \frac{n}{{2}} \\
  \beta_n &= \beta_0 + \frac{\sum(y_i - \bar{y})^2}{2} + \frac{n n_0}{n_0 + n} \frac{(y_i - \mu_0)^2}{2}
\end{align*}
\]

And, the marginal posterior is:
\[p(\sigma^2|y_{i,j}) \sim IG(\alpha_n, \beta_n)\] Now, using the
information, we have sampled from the distribution and then plot a
histogram for the posterior with pdf.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# We have the given prior information as (mu_j, sigma_j^2) ~ N-IG(mu_0, n_0, alpha_0, beta_0)}
\NormalTok{mu_0 =}\StringTok{ }\DecValTok{0}
\NormalTok{n_0 =}\StringTok{ }\FloatTok{0.000001}
\NormalTok{alpha_0 =}\StringTok{ }\FloatTok{0.000001}
\NormalTok{beta_0 =}\StringTok{ }\FloatTok{0.000001}

\CommentTok{# Reading and pre-processing the data }

\NormalTok{f1 =}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"oasis_cross-sectional.csv"}\NormalTok{)}
\NormalTok{f2 =}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"oasis_hippocampus.csv"}\NormalTok{)}
\NormalTok{f =}\StringTok{ }\KeywordTok{merge}\NormalTok{(f1,f2, }\DataTypeTok{by =} \StringTok{'ID'}\NormalTok{)}

\NormalTok{f$group[f$CDR >=}\StringTok{ }\FloatTok{1.0}\NormalTok{] =}\StringTok{ "Dementia"}
\NormalTok{f$group[f$CDR ==}\StringTok{ }\FloatTok{0.5}\NormalTok{] =}\StringTok{ "Mild"}
\NormalTok{f$group[f$CDR ==}\StringTok{ }\FloatTok{0.0}\NormalTok{] =}\StringTok{ "Control"}
\NormalTok{f$group =}\StringTok{ }\KeywordTok{factor}\NormalTok{(f$group, }\DataTypeTok{levels =} \KeywordTok{c}\NormalTok{(}\StringTok{"Dementia"}\NormalTok{, }\StringTok{"Mild"}\NormalTok{, }\StringTok{"Control"}\NormalTok{))}

\CommentTok{# Right Hippocampal volume for each of the three groups}
\NormalTok{dementia_y3 =}\StringTok{ }\NormalTok{f$RightHippoVol[f$group ==}\StringTok{ "Dementia"}\NormalTok{]}
\NormalTok{mild_y2 =}\StringTok{ }\NormalTok{f$RightHippoVol[f$group ==}\StringTok{ "Mild"}\NormalTok{]}
\NormalTok{control_y1 =}\StringTok{ }\NormalTok{f$RightHippoVol[f$group ==}\StringTok{ "Control"}\NormalTok{]}

\CommentTok{#Removing NA entries from the three arrays}
\NormalTok{control_y1 =}\StringTok{ }\NormalTok{control_y1[!}\KeywordTok{is.na}\NormalTok{(control_y1)]}
\NormalTok{mild_y2 =}\StringTok{ }\NormalTok{mild_y2[!}\KeywordTok{is.na}\NormalTok{(mild_y2)]}
\NormalTok{dementia_y3 =}\StringTok{ }\NormalTok{dementia_y3[!}\KeywordTok{is.na}\NormalTok{(dementia_y3)]}

\CommentTok{# We can calculate sample mean, variances, and lengths of the data for each of the three groups}
\NormalTok{mu_y3 =}\StringTok{ }\KeywordTok{mean}\NormalTok{(dementia_y3, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{mu_y2 =}\StringTok{ }\KeywordTok{mean}\NormalTok{(mild_y2, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{mu_y1 =}\StringTok{ }\KeywordTok{mean}\NormalTok{(control_y1, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}

\NormalTok{var_y3 =}\StringTok{ }\KeywordTok{mean}\NormalTok{((dementia_y3 -}\StringTok{ }\NormalTok{mu_y3)^}\DecValTok{2}\NormalTok{, }\DataTypeTok{na.rm=} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{var_y2 =}\StringTok{ }\KeywordTok{mean}\NormalTok{((mild_y2 -}\StringTok{ }\NormalTok{mu_y2)^}\DecValTok{2}\NormalTok{, }\DataTypeTok{na.rm=} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{var_y1 =}\StringTok{ }\KeywordTok{mean}\NormalTok{((control_y1 -}\StringTok{ }\NormalTok{mu_y1)^}\DecValTok{2}\NormalTok{, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}

\NormalTok{n_y3 =}\StringTok{ }\KeywordTok{length}\NormalTok{(dementia_y3)}
\NormalTok{n_y2 =}\StringTok{ }\KeywordTok{length}\NormalTok{(mild_y2)}
\NormalTok{n_y1 =}\StringTok{ }\KeywordTok{length}\NormalTok{(control_y1)}

\CommentTok{#Function definitions }

\CommentTok{# function to get n random samples from a inverse gamma distribution}
\NormalTok{rinvgamma <-}\StringTok{ }\NormalTok{function(num, alpha, beta)\{}
  \NormalTok{inv =}\StringTok{ }\KeywordTok{rgamma}\NormalTok{(}\DataTypeTok{n =} \NormalTok{num, }\DataTypeTok{shape =} \NormalTok{alpha, }\DataTypeTok{rate =} \NormalTok{beta)}
  \NormalTok{inv =}\StringTok{ }\DecValTok{1}\NormalTok{/inv}
  \KeywordTok{return}\NormalTok{(inv)}
\NormalTok{\}}
\CommentTok{# Inverse Gamma PDF}
\NormalTok{dinvgamma <-}\StringTok{ }\NormalTok{function(x, alpha, beta)\{}
  \NormalTok{a =}\StringTok{ }\KeywordTok{exp}\NormalTok{(alpha*}\KeywordTok{log}\NormalTok{(beta) -}\StringTok{ }\KeywordTok{log}\NormalTok{(}\KeywordTok{gamma}\NormalTok{(alpha))) *}\StringTok{ }\NormalTok{x^(-alpha -}\StringTok{ }\DecValTok{1}\NormalTok{) *}\StringTok{ }\KeywordTok{exp}\NormalTok{(-beta /}\StringTok{ }\NormalTok{x)}
  \KeywordTok{return}\NormalTok{(a)}
\NormalTok{\}}

\CommentTok{# Parameters for the posterior N-IG as discussed in class}
\NormalTok{mun_y1 =}\StringTok{ }\NormalTok{(n_0 *}\StringTok{ }\NormalTok{mu_0 +}\StringTok{ }\NormalTok{n_y1*mu_y1)/(n_0 +}\StringTok{ }\NormalTok{n_y1)}
\NormalTok{nn_y1 =}\StringTok{ }\NormalTok{n_0 +}\StringTok{ }\NormalTok{n_y1}
\NormalTok{alphan_y1 =}\StringTok{ }\NormalTok{alpha_0 +}\StringTok{ }\NormalTok{n_y1/}\DecValTok{2}
\NormalTok{betan_y1 =}\StringTok{ }\NormalTok{beta_0 +}\StringTok{ }\NormalTok{(var_y1 *}\StringTok{ }\NormalTok{n_y1 /}\DecValTok{2}\NormalTok{) +}\StringTok{ }\NormalTok{((n_y1*n_0 /}\StringTok{ }\NormalTok{(n_0 +}\StringTok{ }\NormalTok{n_y1))*((mu_y1 -}\StringTok{ }\NormalTok{mu_0)^}\DecValTok{2}\NormalTok{/}\DecValTok{2}\NormalTok{))}

\NormalTok{mun_y2 =}\StringTok{ }\NormalTok{((n_0 *}\StringTok{ }\NormalTok{mu_0) +}\StringTok{ }\NormalTok{n_y2*mu_y2)/(n_0 +}\StringTok{ }\NormalTok{n_y2)}
\NormalTok{nn_y2 =}\StringTok{ }\NormalTok{n_0 +}\StringTok{ }\NormalTok{n_y2}
\NormalTok{alphan_y2 =}\StringTok{ }\NormalTok{alpha_0 +}\StringTok{ }\NormalTok{n_y2/}\DecValTok{2}
\NormalTok{betan_y2 =}\StringTok{ }\NormalTok{beta_0 +}\StringTok{ }\NormalTok{(var_y2 *}\StringTok{ }\NormalTok{n_y2 /}\DecValTok{2}\NormalTok{) +}\StringTok{ }\NormalTok{((n_y2*n_0 /}\StringTok{ }\NormalTok{(n_0 +}\StringTok{ }\NormalTok{n_y2))*((mu_y2 -}\StringTok{ }\NormalTok{mu_0)^}\DecValTok{2}\NormalTok{/}\DecValTok{2}\NormalTok{))}

\NormalTok{mun_y3 =}\StringTok{ }\NormalTok{((n_0 *}\StringTok{ }\NormalTok{mu_0) +}\StringTok{ }\NormalTok{n_y3*mu_y3)/(n_0 +}\StringTok{ }\NormalTok{n_y3)}
\NormalTok{nn_y3 =}\StringTok{ }\NormalTok{n_0 +}\StringTok{ }\NormalTok{n_y3}
\NormalTok{alphan_y3 =}\StringTok{ }\NormalTok{alpha_0 +}\StringTok{ }\NormalTok{n_y3/}\DecValTok{2}
\NormalTok{betan_y3 =}\StringTok{ }\NormalTok{beta_0 +}\StringTok{ }\NormalTok{(var_y3 *}\StringTok{ }\NormalTok{n_y3 /}\DecValTok{2}\NormalTok{) +}\StringTok{ }\NormalTok{((n_y3*n_0 /}\StringTok{ }\NormalTok{(n_0 +}\StringTok{ }\NormalTok{n_y3))*((mu_y3 -}\StringTok{ }\NormalTok{mu_0)^}\DecValTok{2}\NormalTok{/}\DecValTok{2}\NormalTok{))}


\CommentTok{#Part a of Question 4}
\NormalTok{n =}\StringTok{ }\DecValTok{1000000}
\CommentTok{#x = seq(10^5, 7*10^5,10)}
\CommentTok{# We need to plot for only the control group}
\NormalTok{sig =}\StringTok{ }\KeywordTok{rinvgamma}\NormalTok{(n, alphan_y1, betan_y1)}
\NormalTok{p_sig =}\StringTok{ }\KeywordTok{dinvgamma}\NormalTok{(sig, alphan_y1, betan_y1)}

\NormalTok{h =}\StringTok{ }\KeywordTok{hist}\NormalTok{(sig,}\DataTypeTok{main =} \StringTok{"Marginal Posterior Density with Conjugate Prior"}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{"Variance"}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{"Value of posterior"}\NormalTok{,}\DataTypeTok{breaks=} \DecValTok{1000}\NormalTok{, }\DataTypeTok{freq =} \OtherTok{FALSE}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(sig), }\DataTypeTok{col =} \StringTok{"green"}\NormalTok{, }\DataTypeTok{lwd=}\DecValTok{2}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=} \NormalTok{var_y1, }\DataTypeTok{col =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{lwd =} \DecValTok{2}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\StringTok{'topright'}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\StringTok{"Histogram of Variance"}\NormalTok{, }\StringTok{"PDF of posterior"}\NormalTok{, }\StringTok{"Sample Variance"}\NormalTok{), }\DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"green"}\NormalTok{, }\StringTok{"blue"}\NormalTok{), }\DataTypeTok{lwd=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Assignment_1_files/figure-latex/unnamed-chunk-1-1.pdf}

\subsubsection{b)}\label{b-1}

Solution: Here, we want to find the marginal posterior distrubution
\(p(\mu_j | y_{ij})\). Next, we want to plot this for each of the groups
as well. Finally, we will draw the sample means of each group as a
vertical line. After looking for the marginal distribution of N-IG from
Wikipedia, we found that it can be written as a t-distribution with a
certain shift and scaling parameter. We will be using dt function in R
as the student-t disctribution. Essentially, we have
\[p(\mu_j | y_{ij}) \sim t_{2\alpha_{n}}(\mu_j \cdot scale + shift)\]
Here,
\[Scale = \sqrt{\frac{\beta_n}{\alpha_n n_n}} \qquad Shift = \mu_n\] It
can also be written as
\[T_{2\alpha} (\mu_p, \frac{\beta_p}{\alpha_p n_p})\]

The code is as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n =}\StringTok{ }\DecValTok{1000000}

\CommentTok{#For the control dataset, the shift, scale and degree of freedom parameters are}
\NormalTok{scale1 =}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(alphan_y1 *}\StringTok{ }\NormalTok{nn_y1/betan_y1)}
\NormalTok{shift1 =}\StringTok{ }\NormalTok{mun_y1}
\NormalTok{df =}\StringTok{ }\DecValTok{2}\NormalTok{*alphan_y1}

\NormalTok{y =}\StringTok{ }\KeywordTok{seq}\NormalTok{(mu_y1 -}\StringTok{ }\DecValTok{2000}\NormalTok{, mu_y1 +}\StringTok{ }\DecValTok{2000}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\CommentTok{# Pdf of y from dt}
\NormalTok{pdf_y =}\StringTok{ }\KeywordTok{dt}\NormalTok{(y, df, }\DataTypeTok{ncp =} \NormalTok{shift1)}
\NormalTok{pdf_y =}\StringTok{ }\NormalTok{pdf_y/scale1}
\KeywordTok{plot}\NormalTok{(y,pdf_y, }\DataTypeTok{col =}\StringTok{"green"}\NormalTok{, }\DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\DecValTok{2000}\NormalTok{, }\DecValTok{6000}\NormalTok{), }\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{0.12}\NormalTok{),}\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{, }\DataTypeTok{main =} \StringTok{"Marginal Posterior over mean using conjugate prior"}\NormalTok{, }\DataTypeTok{xlab =} \KeywordTok{expression}\NormalTok{(mu[j]), }\DataTypeTok{ylab=} \StringTok{"p-value"}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\NormalTok{mu_y1, }\DataTypeTok{col =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{lty=}\DecValTok{5}\NormalTok{)}

\CommentTok{# For the Mild Dataset, we follow the same process}
\NormalTok{shift2 =}\StringTok{ }\NormalTok{mun_y2}
\NormalTok{scale2 =}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(alphan_y2 *}\StringTok{ }\NormalTok{nn_y2/betan_y2)}
\NormalTok{df =}\StringTok{ }\DecValTok{2}\NormalTok{*alphan_y2}

\NormalTok{y =}\StringTok{ }\KeywordTok{seq}\NormalTok{(mu_y1 -}\StringTok{ }\DecValTok{2000}\NormalTok{, mu_y1 +}\StringTok{ }\DecValTok{2000}\NormalTok{, }\DecValTok{10}\NormalTok{)}

\NormalTok{pdf_y =}\StringTok{ }\KeywordTok{dt}\NormalTok{(y, df, }\DataTypeTok{ncp =} \NormalTok{shift2)}
\NormalTok{pdf_y =}\StringTok{ }\NormalTok{pdf_y/scale2}
\KeywordTok{lines}\NormalTok{(y, pdf_y, }\DataTypeTok{col=}\StringTok{"yellow"}\NormalTok{, }\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\NormalTok{mu_y2, }\DataTypeTok{col =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{lty=}\DecValTok{5}\NormalTok{)}

\CommentTok{# For the dementia Dataset, we follow the same process again}
\NormalTok{shift3 =}\StringTok{ }\NormalTok{mun_y3}
\NormalTok{scale3 =}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(alphan_y3 *}\StringTok{ }\NormalTok{nn_y3/betan_y3)}
\NormalTok{df =}\StringTok{ }\DecValTok{2}\NormalTok{*alphan_y3}

\CommentTok{#Now, we will use the in-built t-distribution random number generator}
\NormalTok{y =}\StringTok{ }\KeywordTok{seq}\NormalTok{(mu_y1 -}\StringTok{ }\DecValTok{2000}\NormalTok{, mu_y1 +}\StringTok{ }\DecValTok{2000}\NormalTok{, }\DecValTok{10}\NormalTok{)}

\NormalTok{pdf_y =}\StringTok{ }\KeywordTok{dt}\NormalTok{(y, df, }\DataTypeTok{ncp =} \NormalTok{shift3)}
\NormalTok{pdf_y =}\StringTok{ }\NormalTok{pdf_y/scale3}
\KeywordTok{lines}\NormalTok{(y, pdf_y, }\DataTypeTok{col=}\StringTok{"black"}\NormalTok{, }\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\NormalTok{mu_y3, }\DataTypeTok{col =} \StringTok{"grey"}\NormalTok{, }\DataTypeTok{lty=}\DecValTok{5}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\StringTok{'topright'}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\StringTok{"Control"}\NormalTok{, }\StringTok{"Sample mean - control"}\NormalTok{, }\StringTok{"Mild"}\NormalTok{, }\StringTok{"Sample mean - mild"}\NormalTok{,}\StringTok{"Dementia"}\NormalTok{, }\StringTok{"Sample mean - dementia"}\NormalTok{), }\DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"green"}\NormalTok{, }\StringTok{"blue"}\NormalTok{, }\StringTok{"yellow"}\NormalTok{, }\StringTok{"red"}\NormalTok{, }\StringTok{"black"}\NormalTok{, }\StringTok{"grey"}\NormalTok{), }\DataTypeTok{lty =} \DecValTok{1}\NormalTok{:}\DecValTok{2}\NormalTok{, }\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Assignment_1_files/figure-latex/unnamed-chunk-2-1.pdf}

\subsubsection{c)}\label{c}

Solution: Here, we have the random variable \(d_{12} = \mu_1 - \mu_2\).
Now, we need to find the conditional density function i.e.
\(p(d_{12}| \sigma_1^2, \sigma_2^2, y_{ij})\) So, we know that the
difference between two gaussians is also a gaussian with a different
mean and variance, the process is as follows: \[
\begin{align*}
p(d_{12}| \sigma_1^2, \sigma_2^2, y_{ij}) &\sim N(\mu_1,\sigma_1^2) - N(\mu_2,\sigma_2^2) \\
p(d_{12}| \sigma_1^2, \sigma_2^2, y_{ij}) &\sim N(\mu_1 - \mu_2, \frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2})
\end{align*}
\] So, now we will code it for these values, the process is as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n =}\StringTok{ }\DecValTok{1000000}
\CommentTok{#Calculate the parameters for the new distribution}

\CommentTok{#Datesets 1 and 2}
\NormalTok{new_mean =}\StringTok{ }\NormalTok{mun_y1 -}\StringTok{ }\NormalTok{mun_y2}
\NormalTok{var1 =}\StringTok{ }\KeywordTok{rinvgamma}\NormalTok{(n, alphan_y1, betan_y1)}
\NormalTok{var2 =}\StringTok{ }\KeywordTok{rinvgamma}\NormalTok{(n, alphan_y2, betan_y2)}
\NormalTok{new_sigma =}\StringTok{ }\KeywordTok{sqrt}\NormalTok{((var1/nn_y1)+(var2/nn_y2))}
\NormalTok{sampled_values =}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n, new_mean, new_sigma)}

\CommentTok{#Now we will calculate the mentioned probability}
\NormalTok{p_req1 =}\StringTok{ }\KeywordTok{sum}\NormalTok{(sampled_values <}\StringTok{ }\DecValTok{0}\NormalTok{)/}\KeywordTok{length}\NormalTok{(sampled_values)}
\CommentTok{#hist(sampled_values)}
\CommentTok{#Datasets 1 and 3}
\NormalTok{new_mean =}\StringTok{ }\NormalTok{mun_y1 -}\StringTok{ }\NormalTok{mun_y3}

\NormalTok{var1 =}\StringTok{ }\KeywordTok{rinvgamma}\NormalTok{(n, alphan_y1, betan_y1)}
\NormalTok{var3 =}\StringTok{ }\KeywordTok{rinvgamma}\NormalTok{(n, alphan_y3, betan_y3)}
\NormalTok{new_sigma =}\StringTok{ }\KeywordTok{sqrt}\NormalTok{((var1/nn_y1)+}\StringTok{ }\NormalTok{(var3/nn_y3))}

\NormalTok{sampled_values =}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n, new_mean, new_sigma)}

\CommentTok{#Now we will calculate the mentioned probability}
\NormalTok{p_req2 =}\StringTok{ }\KeywordTok{sum}\NormalTok{(sampled_values <}\StringTok{ }\DecValTok{0}\NormalTok{)/}\KeywordTok{length}\NormalTok{(sampled_values)}

\CommentTok{#Datasets 2 and 3}
\NormalTok{new_mean =}\StringTok{ }\NormalTok{mun_y2 -}\StringTok{ }\NormalTok{mun_y3}
\NormalTok{var2 =}\StringTok{ }\KeywordTok{rinvgamma}\NormalTok{(n, alphan_y2, betan_y2)}
\NormalTok{var3 =}\StringTok{ }\KeywordTok{rinvgamma}\NormalTok{(n, alphan_y3, betan_y3)}
\NormalTok{new_sigma =}\StringTok{ }\KeywordTok{sqrt}\NormalTok{((var2/nn_y2)+}\StringTok{ }\NormalTok{(var3/nn_y3))}
\CommentTok{#new_sigma = sqrt(var2+ var3)}
\NormalTok{sampled_values =}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n, new_mean, new_sigma)}

\CommentTok{#Now we will calculate the mentioned probability}
\NormalTok{p_req3 =}\StringTok{ }\KeywordTok{sum}\NormalTok{(sampled_values <}\StringTok{ }\DecValTok{0}\NormalTok{)/}\KeywordTok{length}\NormalTok{(sampled_values)}
\CommentTok{#hist(sampled_values)}
\CommentTok{#print the values out}
\NormalTok{p_req1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p_req2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p_req3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.031885
\end{verbatim}

So, we have the results written above as the output from the R code. The
values are \(p(d_{12} < 0| \sigma_1^2, \sigma_2^2, y_{ij})\),
\(p(d_{13} < 0| \sigma_1^2, \sigma_2^2, y_{ij})\), and
\(p(d_{23} < 0| \sigma_1^2, \sigma_2^2, y_{ij})\). We can rewrite these
probabilities i.e \(p(d_{12} < 0| \sigma_1^2, \sigma_2^2, y_{ij})\) as
\(p(\mu_1 < \mu_2| \sigma_1^2, \sigma_2^2, y_{ij})\). Similarly,
\(p(d_{13} < 0| \sigma_1^2, \sigma_3^2, y_{ij})\) as
\(p(\mu_1 < \mu_3| \sigma_1^2, \sigma_3^2, y_{ij})\) and
\(p(d_{23} < 0| \sigma_2^2, \sigma_3^2, y_{ij})\) as
\(p(\mu_2 < \mu_3| \sigma_2^2, \sigma_3^2, y_{ij})\). For the \(d_{12}\)
and \(d_{13}\) case, we are getting infinitesimal near to zero
probability, this can be verified from the plots in 4b where we can see
that the overlapped region is very small for the (Control, Mild) and
(Control, Dementia) pairs. But for the (Mild, Dementia) pair, we have
much more signifcant overlap and thus the probability is higher.

\subsubsection{d)}\label{d}

Solution: Here, we just need to perform t-test on the three
combinations. The tests are written as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{t1 =}\StringTok{ }\KeywordTok{t.test}\NormalTok{(control_y1, mild_y2, }\DataTypeTok{alternative=}\StringTok{'greater'}\NormalTok{)}
\NormalTok{t2 =}\StringTok{ }\KeywordTok{t.test}\NormalTok{(control_y1, dementia_y3, }\DataTypeTok{alternative =} \StringTok{'greater'}\NormalTok{)}
\NormalTok{t3 =}\StringTok{ }\KeywordTok{t.test}\NormalTok{(mild_y2, dementia_y3, }\DataTypeTok{alternative =} \StringTok{'greater'}\NormalTok{)}
\NormalTok{t1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Welch Two Sample t-test
## 
## data:  control_y1 and mild_y2
## t = 7.2205, df = 131.41, p-value = 1.868e-11
## alternative hypothesis: true difference in means is greater than 0
## 95 percent confidence interval:
##  455.3136      Inf
## sample estimates:
## mean of x mean of y 
##  3880.606  3289.735
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{t2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Welch Two Sample t-test
## 
## data:  control_y1 and dementia_y3
## t = 6.4866, df = 31.197, p-value = 1.504e-07
## alternative hypothesis: true difference in means is greater than 0
## 95 percent confidence interval:
##  633.216     Inf
## sample estimates:
## mean of x mean of y 
##  3880.606  3023.360
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{t3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Welch Two Sample t-test
## 
## data:  mild_y2 and dementia_y3
## t = 1.8925, df = 39.069, p-value = 0.03293
## alternative hypothesis: true difference in means is greater than 0
## 95 percent confidence interval:
##  29.23099      Inf
## sample estimates:
## mean of x mean of y 
##  3289.735  3023.360
\end{verbatim}

The results of the t-test are attached above. In terms of statistics,
the probability of finding the observed, or more extreme results when
the null hypothesis (\(H_0\)) for a study in question is true (ref.
statsdirect.com). In the context of our problem, p-value can be
considered as the probability that the difference of two means is
negative. It is the also considered as the frequentist or likelihood
approach to estimate the difference. Now, as far as the result goes, the
probability obtained in c part matches with the p-values obtained
through t tests. In other words, it is a frequentist approach
verification of our bayesian analysis.

\subsection{Question 5 - Jeffrey's
Prior}\label{question-5---jeffreys-prior}

\subsubsection{(a)}\label{a-3}

We have the joint posterior density function for Jeffrey Priors as (ref.
Baysian Data Analysis by Gelman pp.65)
\[ p(\mu_j,\sigma_j^2|y_{i,j}) = \sigma_j^{-n-2} exp\left(-\frac{(n-1)s^2+n(\bar{y}-\mu_j)^2}{2\sigma_j^2}\right)\]

For the formula above, we have, \[
\begin{align*}
\text{Sample Mean } \bar{y} &= \frac{1}{n}\sum_{i=1}^{n} y_i \\
\text{Sample Variance } s^2 &= \frac{1}{n}\sum_{i=1}^{n} (y_i - \bar{y})^2
\end{align*}
\] Also, we have the marginal posterior (saw online) as:
\[p(\sigma_j^2|y_{i,j}) = IG\left(\frac{n-1}{2},\frac{(n-1)s^2}{2}\right)\]

We have to follow all the steps from 4th question again, the (a) part of
the above question is coded as follows. We will reuse some of the
variables like sample mean, variance from our previous calculations.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# We will calculate the new alphas and betas here}
\NormalTok{alphan_y1 =}\StringTok{ }\NormalTok{(n_y1 -}\StringTok{ }\DecValTok{1}\NormalTok{)/}\DecValTok{2}
\NormalTok{betan_y1 =}\StringTok{ }\NormalTok{var_y1*(n_y1}\DecValTok{-1}\NormalTok{)/}\DecValTok{2}

\NormalTok{alphan_y2 =}\StringTok{ }\NormalTok{(n_y2}\DecValTok{-1}\NormalTok{)/}\DecValTok{2}
\NormalTok{betan_y2 =}\StringTok{ }\NormalTok{var_y2*(n_y2}\DecValTok{-1}\NormalTok{)/}\DecValTok{2}

\NormalTok{alphan_y3 =}\StringTok{ }\NormalTok{(n_y3 -}\StringTok{ }\DecValTok{1}\NormalTok{)/}\DecValTok{2}
\NormalTok{betan_y3 =}\StringTok{ }\NormalTok{var_y3*(n_y3}\DecValTok{-1}\NormalTok{)/}\DecValTok{2}

\CommentTok{#Part a of Question 4}
\NormalTok{n =}\StringTok{ }\DecValTok{1000000}
\CommentTok{#x = seq(10^5, 7*10^5,10)}
\CommentTok{# We need to plot for only the control group}
\NormalTok{sig =}\StringTok{ }\KeywordTok{rinvgamma}\NormalTok{(n, alphan_y1, betan_y1)}
\NormalTok{p_sig =}\StringTok{ }\KeywordTok{dinvgamma}\NormalTok{(sig, alphan_y1, betan_y1)}

\NormalTok{h =}\StringTok{ }\KeywordTok{hist}\NormalTok{(sig,}\DataTypeTok{main =} \StringTok{"Marginal Posterior Density function with Conjugate Prior"}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{"Variance"}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{"Value of posterior"}\NormalTok{,}\DataTypeTok{breaks=} \DecValTok{1000}\NormalTok{, }\DataTypeTok{freq =} \OtherTok{FALSE}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(sig), }\DataTypeTok{col =} \StringTok{"green"}\NormalTok{, }\DataTypeTok{lwd=}\DecValTok{2}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=} \NormalTok{var_y1, }\DataTypeTok{col =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{lwd =} \DecValTok{2}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\StringTok{'topright'}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\StringTok{"Histogram of Variance"}\NormalTok{, }\StringTok{"PDF of posterior"}\NormalTok{, }\StringTok{"Sample Variance"}\NormalTok{), }\DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"green"}\NormalTok{, }\StringTok{"blue"}\NormalTok{), }\DataTypeTok{lwd=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Assignment_1_files/figure-latex/unnamed-chunk-5-1.pdf}

From the plot of marginal posterior density function above, we can see
that it looks similar to the plot in question 4a of conjugate prior.

\subsection{b)}\label{b-2}

Now, repeating the b part from the question 4. Again from the same
reference as before, the marginal posterior for the jeffrey prior are:
\[p(\mu_j|y_{i,j}) \sim t_{n-1}(scale* \mu_j + shift)\] Here, the scale
and shift are \[Scale = \frac{s}{\sqrt{n-1}} \qquad Shift = \bar{y}\] It
can also be written as \(T_{n-1} (\bar{y}, \frac{s^2}{n})\). Code for b
part is as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n =}\StringTok{ }\DecValTok{1000000}

\CommentTok{#For the control dataset, the shift, scale and degree of freedom parameters are}
\NormalTok{scale1 =}\StringTok{ }\KeywordTok{sqrt}\NormalTok{((nn_y1}\DecValTok{-1}\NormalTok{)/var_y1)}
\NormalTok{shift1 =}\StringTok{ }\NormalTok{mu_y1}
\NormalTok{df =}\StringTok{ }\NormalTok{nn_y1 -}\StringTok{ }\DecValTok{1}

\NormalTok{y =}\StringTok{ }\KeywordTok{seq}\NormalTok{(mu_y1 -}\StringTok{ }\DecValTok{2000}\NormalTok{, mu_y1 +}\StringTok{ }\DecValTok{2000}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\CommentTok{# Pdf of y from dt}
\NormalTok{pdf_y =}\StringTok{ }\KeywordTok{dt}\NormalTok{(y, df, }\DataTypeTok{ncp =} \NormalTok{shift1)}
\NormalTok{pdf_y =}\StringTok{ }\NormalTok{pdf_y/scale1}
\KeywordTok{plot}\NormalTok{(y,pdf_y, }\DataTypeTok{col =}\StringTok{"green"}\NormalTok{, }\DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\DecValTok{2000}\NormalTok{, }\DecValTok{6000}\NormalTok{), }\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{0.12}\NormalTok{),}\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{, }\DataTypeTok{main =} \StringTok{"Marginal Posterior over mean using conjugate prior"}\NormalTok{, }\DataTypeTok{xlab =} \KeywordTok{expression}\NormalTok{(mu[j]), }\DataTypeTok{ylab=} \StringTok{"p-value"}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\NormalTok{mu_y1, }\DataTypeTok{col =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{lty=}\DecValTok{5}\NormalTok{)}

\CommentTok{# For the Mild Dataset, we follow the same process}
\NormalTok{scale2 =}\StringTok{ }\KeywordTok{sqrt}\NormalTok{((nn_y2}\DecValTok{-1}\NormalTok{)/var_y2)}
\NormalTok{shift2 =}\StringTok{ }\NormalTok{mu_y2}
\NormalTok{df =}\StringTok{ }\NormalTok{nn_y2 -}\StringTok{ }\DecValTok{1}

\NormalTok{y =}\StringTok{ }\KeywordTok{seq}\NormalTok{(mu_y1 -}\StringTok{ }\DecValTok{2000}\NormalTok{, mu_y1 +}\StringTok{ }\DecValTok{2000}\NormalTok{, }\DecValTok{10}\NormalTok{)}

\NormalTok{pdf_y =}\StringTok{ }\KeywordTok{dt}\NormalTok{(y, df, }\DataTypeTok{ncp =} \NormalTok{shift2)}
\NormalTok{pdf_y =}\StringTok{ }\NormalTok{pdf_y/scale2}
\KeywordTok{lines}\NormalTok{(y, pdf_y, }\DataTypeTok{col=}\StringTok{"yellow"}\NormalTok{, }\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\NormalTok{mu_y2, }\DataTypeTok{col =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{lty=}\DecValTok{5}\NormalTok{)}

\CommentTok{# For the dementia Dataset, we follow the same process again}
\NormalTok{scale3 =}\StringTok{ }\KeywordTok{sqrt}\NormalTok{((nn_y3}\DecValTok{-1}\NormalTok{)/var_y3)}
\NormalTok{shift3 =}\StringTok{ }\NormalTok{mu_y3}
\NormalTok{df =}\StringTok{ }\NormalTok{nn_y3 -}\StringTok{ }\DecValTok{1}
\CommentTok{#Now, we will use the in-built t-distribution random number generator}
\NormalTok{y =}\StringTok{ }\KeywordTok{seq}\NormalTok{(mu_y1 -}\StringTok{ }\DecValTok{2000}\NormalTok{, mu_y1 +}\StringTok{ }\DecValTok{2000}\NormalTok{, }\DecValTok{10}\NormalTok{)}

\NormalTok{pdf_y =}\StringTok{ }\KeywordTok{dt}\NormalTok{(y, df, }\DataTypeTok{ncp =} \NormalTok{shift3)}
\NormalTok{pdf_y =}\StringTok{ }\NormalTok{pdf_y/scale3}
\KeywordTok{lines}\NormalTok{(y, pdf_y, }\DataTypeTok{col=}\StringTok{"black"}\NormalTok{, }\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\NormalTok{mu_y3, }\DataTypeTok{col =} \StringTok{"grey"}\NormalTok{, }\DataTypeTok{lty=}\DecValTok{5}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\StringTok{'topright'}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\StringTok{"Control"}\NormalTok{, }\StringTok{"Sample mean - control"}\NormalTok{, }\StringTok{"Mild"}\NormalTok{, }\StringTok{"Sample mean - mild"}\NormalTok{,}\StringTok{"Dementia"}\NormalTok{, }\StringTok{"Sample mean - dementia"}\NormalTok{), }\DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"green"}\NormalTok{, }\StringTok{"blue"}\NormalTok{, }\StringTok{"yellow"}\NormalTok{, }\StringTok{"red"}\NormalTok{, }\StringTok{"black"}\NormalTok{, }\StringTok{"grey"}\NormalTok{), }\DataTypeTok{lty =} \DecValTok{1}\NormalTok{:}\DecValTok{2}\NormalTok{, }\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Assignment_1_files/figure-latex/unnamed-chunk-6-1.pdf}

\subsubsection{c)}\label{c-1}

Solution: Here, we have the random variable \(d_{12} = \mu_1 - \mu_2\).
Now, we need to find the conditional density function i.e.
\(p(d_{12}, \sigma_1^2, \sigma_2^2, y_{ij})\) So, we know that the
difference between two gaussians is also a gaussian with a different
mean and variance, the process is as follows: \[
\begin{align*}
p(d_{12}, \sigma_1^2, \sigma_2^2, y_{ij}) &\sim N(\mu_1,\sigma_1^2) - N(\mu_2,\sigma_2^2) \\
p(d_{12}, \sigma_1^2, \sigma_2^2, y_{ij}) &\sim N(\mu_1 - \mu_2, \frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2})
\end{align*}
\] The code to calculate is as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n =}\StringTok{ }\DecValTok{1000000}
\CommentTok{#Calculate the parameters for the new distribution}

\CommentTok{#Datesets 1 and 2}
\NormalTok{new_mean =}\StringTok{ }\NormalTok{mun_y1 -}\StringTok{ }\NormalTok{mun_y2}
\NormalTok{var1 =}\StringTok{ }\KeywordTok{rinvgamma}\NormalTok{(n, alphan_y1, betan_y1)}
\NormalTok{var2 =}\StringTok{ }\KeywordTok{rinvgamma}\NormalTok{(n, alphan_y2, betan_y2)}
\NormalTok{new_sigma =}\StringTok{ }\KeywordTok{sqrt}\NormalTok{((var1/nn_y1)+(var2/nn_y2))}
\NormalTok{sampled_values =}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n, new_mean, new_sigma)}

\CommentTok{#Now we will calculate the mentioned probability}
\NormalTok{p_req1 =}\StringTok{ }\KeywordTok{sum}\NormalTok{(sampled_values <}\StringTok{ }\DecValTok{0}\NormalTok{)/}\KeywordTok{length}\NormalTok{(sampled_values)}

\CommentTok{#Datasets 1 and 3}
\NormalTok{new_mean =}\StringTok{ }\NormalTok{mun_y1 -}\StringTok{ }\NormalTok{mun_y3}

\NormalTok{var1 =}\StringTok{ }\KeywordTok{rinvgamma}\NormalTok{(n, alphan_y1, betan_y1)}
\NormalTok{var3 =}\StringTok{ }\KeywordTok{rinvgamma}\NormalTok{(n, alphan_y3, betan_y3)}
\NormalTok{new_sigma =}\StringTok{ }\KeywordTok{sqrt}\NormalTok{((var1/nn_y1)+}\StringTok{ }\NormalTok{(var3/nn_y3))}

\NormalTok{sampled_values =}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n, new_mean, new_sigma)}

\CommentTok{#Now we will calculate the mentioned probability}
\NormalTok{p_req2 =}\StringTok{ }\KeywordTok{sum}\NormalTok{(sampled_values <}\StringTok{ }\DecValTok{0}\NormalTok{)/}\KeywordTok{length}\NormalTok{(sampled_values)}

\CommentTok{#Datasets 2 and 3}
\NormalTok{new_mean =}\StringTok{ }\NormalTok{mun_y2 -}\StringTok{ }\NormalTok{mun_y3}
\NormalTok{var2 =}\StringTok{ }\KeywordTok{rinvgamma}\NormalTok{(n, alphan_y2, betan_y2)}
\NormalTok{var3 =}\StringTok{ }\KeywordTok{rinvgamma}\NormalTok{(n, alphan_y3, betan_y3)}
\NormalTok{new_sigma =}\StringTok{ }\KeywordTok{sqrt}\NormalTok{((var2/nn_y2)+}\StringTok{ }\NormalTok{(var3/nn_y3))}
\CommentTok{#new_sigma = sqrt(var2+ var3)}
\NormalTok{sampled_values =}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n, new_mean, new_sigma)}

\CommentTok{#Now we will calculate the mentioned probability}
\NormalTok{p_req3 =}\StringTok{ }\KeywordTok{sum}\NormalTok{(sampled_values <}\StringTok{ }\DecValTok{0}\NormalTok{)/}\KeywordTok{length}\NormalTok{(sampled_values)}
\NormalTok{p_req1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p_req2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p_req3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.031975
\end{verbatim}

So, we have the results written above as the output from the R code. The
values are \(p(d_{12} < 0| \sigma_1^2, \sigma_2^2, y_{ij})\),
\(p(d_{13} < 0| \sigma_1^2, \sigma_2^2, y_{ij})\), and
\(p(d_{23} < 0| \sigma_1^2, \sigma_2^2, y_{ij})\). We can rewrite these
probabilities i.e \(p(d_{12} < 0| \sigma_1^2, \sigma_2^2, y_{ij})\) as
\(p(\mu_1 < \mu_2| \sigma_1^2, \sigma_2^2, y_{ij})\). Similarly,
\(p(d_{13} < 0| \sigma_1^2, \sigma_3^2, y_{ij})\) as
\(p(\mu_1 < \mu_3| \sigma_1^2, \sigma_3^2, y_{ij})\) and
\(p(d_{23} < 0| \sigma_2^2, \sigma_3^2, y_{ij})\) as
\(p(\mu_2 < \mu_3| \sigma_2^2, \sigma_3^2, y_{ij})\). For the \(d_{12}\)
and \(d_{13}\) case, we are getting infinitesimal near to zero
probability, this can be verified from the plots in 4b where we can see
that the overlapped region is very small for the (Control, Mild) and
(Control, Dementia) pairs. But for the (Mild, Dementia) pair, we have
much more signifcant overlap and thus the probability is higher.

\subsubsection{d) Relationship between Jeffrey's prior and the conjugate
prior}\label{d-relationship-between-jeffreys-prior-and-the-conjugate-prior}

So, from the 4th ques, we have the conjugate prior as,
\[p(\mu_j, \sigma_j^2) \sim N-IG(\mu_0, n_0, \alpha_0, \beta_0)\]

And, we have the jeffrey prior as
\[p(\mu_j, \sigma_j^2) \sim \propto \frac{1}{\sigma^2}\]

Also, as discussed in class, we know that the Jeffrey prior is the
limiting case of the conjugate prior i.e.~we can say, the pdf of
conjugate would look something like,

\[p(\mu_j, \sigma_j^2) \sim N-IG(0,0,0,0)\] This is essentially the pdf
of Jefferey prior.

Now, from the plots above, we can safely infer that the posteriors
obtained when using Jeffrey's prior are almost identical to the
posteriors obtained using the conjugate prior. The reason is very
simple, as we discussed above, in the limiting case, conjugate prior is
similar to the jeffrey's prior and it becomes flatter
(non-informativeness increases). So, here the parameters of the
conjugate Normal-Inverse Gamma prior are very small (in the order of
\(10^{-6}\)), so we are seeing very closely similar plots and results
for the Jeffrey and Conjugate priors.

\subsection{Question 6}\label{question-6}

Most of this question is similar to the procedure in 4th question except
here we know the priors for the distribution in form of values. As
discussed in the class, the initial parameters for this N-IG will be, \[
\begin{align*}
n_0 &= 127 \\
\mu_0 &= 2133 \\
\sigma_0 &= 279 \\
\alpha_0 &= \frac{n_0}{2} = 63.5 \\
\beta_0 &= \frac{n_0 \sigma_0^2}{2} = 4942903.5
\end{align*}
\]

The code is as follows:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Most of the variables like sample mean, sample variance etc. can be reused but since the initial parameters are changed, we have to calculate the final parameters for the posterior N-IG again}
\NormalTok{n_0 =}\StringTok{ }\DecValTok{127}
\NormalTok{mu_0 =}\StringTok{ }\DecValTok{2133}
\NormalTok{sigma_0 =}\StringTok{ }\DecValTok{279}
\NormalTok{alpha_0 =}\StringTok{ }\FloatTok{63.5}
\NormalTok{beta_0 =}\StringTok{ }\NormalTok{n_0 *}\StringTok{ }\NormalTok{sigma_0^}\DecValTok{2}\NormalTok{/}\DecValTok{2}

\CommentTok{# Parameters for the posterior N-IG as discussed in class}
\NormalTok{mun_y1 =}\StringTok{ }\NormalTok{(n_0 *}\StringTok{ }\NormalTok{mu_0 +}\StringTok{ }\NormalTok{n_y1*mu_y1)/(n_0 +}\StringTok{ }\NormalTok{n_y1)}
\NormalTok{nn_y1 =}\StringTok{ }\NormalTok{n_0 +}\StringTok{ }\NormalTok{n_y1}
\NormalTok{alphan_y1 =}\StringTok{ }\NormalTok{alpha_0 +}\StringTok{ }\NormalTok{n_y1/}\DecValTok{2}
\NormalTok{betan_y1 =}\StringTok{ }\NormalTok{beta_0 +}\StringTok{ }\NormalTok{(var_y1 *}\StringTok{ }\NormalTok{n_y1 /}\DecValTok{2}\NormalTok{) +}\StringTok{ }\NormalTok{((n_y1*n_0 /}\StringTok{ }\NormalTok{(n_0 +}\StringTok{ }\NormalTok{n_y1))*((mu_y1 -}\StringTok{ }\NormalTok{mu_0)^}\DecValTok{2}\NormalTok{/}\DecValTok{2}\NormalTok{))}

\NormalTok{mun_y2 =}\StringTok{ }\NormalTok{((n_0 *}\StringTok{ }\NormalTok{mu_0) +}\StringTok{ }\NormalTok{n_y2*mu_y2)/(n_0 +}\StringTok{ }\NormalTok{n_y2)}
\NormalTok{nn_y2 =}\StringTok{ }\NormalTok{n_0 +}\StringTok{ }\NormalTok{n_y2}
\NormalTok{alphan_y2 =}\StringTok{ }\NormalTok{alpha_0 +}\StringTok{ }\NormalTok{n_y2/}\DecValTok{2}
\NormalTok{betan_y2 =}\StringTok{ }\NormalTok{beta_0 +}\StringTok{ }\NormalTok{(var_y2 *}\StringTok{ }\NormalTok{n_y2 /}\DecValTok{2}\NormalTok{) +}\StringTok{ }\NormalTok{((n_y2*n_0 /}\StringTok{ }\NormalTok{(n_0 +}\StringTok{ }\NormalTok{n_y2))*((mu_y2 -}\StringTok{ }\NormalTok{mu_0)^}\DecValTok{2}\NormalTok{/}\DecValTok{2}\NormalTok{))}

\NormalTok{mun_y3 =}\StringTok{ }\NormalTok{((n_0 *}\StringTok{ }\NormalTok{mu_0) +}\StringTok{ }\NormalTok{n_y3*mu_y3)/(n_0 +}\StringTok{ }\NormalTok{n_y3)}
\NormalTok{nn_y3 =}\StringTok{ }\NormalTok{n_0 +}\StringTok{ }\NormalTok{n_y3}
\NormalTok{alphan_y3 =}\StringTok{ }\NormalTok{alpha_0 +}\StringTok{ }\NormalTok{n_y3/}\DecValTok{2}
\NormalTok{betan_y3 =}\StringTok{ }\NormalTok{beta_0 +}\StringTok{ }\NormalTok{(var_y3 *}\StringTok{ }\NormalTok{n_y3 /}\DecValTok{2}\NormalTok{) +}\StringTok{ }\NormalTok{((n_y3*n_0 /}\StringTok{ }\NormalTok{(n_0 +}\StringTok{ }\NormalTok{n_y3))*((mu_y3 -}\StringTok{ }\NormalTok{mu_0)^}\DecValTok{2}\NormalTok{/}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\subsubsection{a)}\label{a-4}

Solution: The code to plot the histogram, pdf and sample variance is
written below.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Part a of Question 4 repeated}
\NormalTok{n =}\StringTok{ }\DecValTok{1000000}
\CommentTok{# We need to plot for only the control group}
\NormalTok{sig =}\StringTok{ }\KeywordTok{rinvgamma}\NormalTok{(n, alphan_y1, betan_y1)}
\NormalTok{p_sig =}\StringTok{ }\KeywordTok{dinvgamma}\NormalTok{(sig, alphan_y1, betan_y1)}

\NormalTok{h =}\StringTok{ }\KeywordTok{hist}\NormalTok{(sig,}\DataTypeTok{main =} \StringTok{"Marginal Posterior Density"} \NormalTok{,}\DataTypeTok{xlab =} \StringTok{"Variance"}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{"Value of posterior"}\NormalTok{, }\DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\DecValTok{200000}\NormalTok{, }\DecValTok{1500000}\NormalTok{), }\DataTypeTok{breaks=} \DecValTok{3000}\NormalTok{,}\DataTypeTok{freq =} \OtherTok{FALSE}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{density}\NormalTok{(sig), }\DataTypeTok{col =} \StringTok{"green"}\NormalTok{, }\DataTypeTok{lwd=}\DecValTok{2}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=} \NormalTok{var_y1, }\DataTypeTok{col =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{lwd =} \DecValTok{2}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\StringTok{'topright'}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\StringTok{"Histogram of Variance"}\NormalTok{, }\StringTok{"PDF of posterior"}\NormalTok{, }\StringTok{"Sample Variance"}\NormalTok{), }\DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"green"}\NormalTok{, }\StringTok{"blue"}\NormalTok{), }\DataTypeTok{lwd=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Assignment_1_files/figure-latex/unnamed-chunk-9-1.pdf}

From the above plot, we can conclude that the sample of the
\(\sigma_j^2\) parameter distribution is shifted a lot as compared to
the sample variance.

\subsection{b)}\label{b-3}

Solution: Similar to the b part of question 4, the code is as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{### Part b of question 4 repeated}
\NormalTok{n =}\StringTok{ }\DecValTok{1000000}

\CommentTok{#For the control dataset, the shift, scale and degree of freedom parameters are}
\NormalTok{scale1 =}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(alphan_y1 *}\StringTok{ }\NormalTok{nn_y1/betan_y1)}
\NormalTok{shift1 =}\StringTok{ }\NormalTok{mun_y1}
\NormalTok{df =}\StringTok{ }\DecValTok{2}\NormalTok{*alphan_y1}

\NormalTok{y =}\StringTok{ }\KeywordTok{seq}\NormalTok{(mu_y1 -}\StringTok{ }\DecValTok{2000}\NormalTok{, mu_y1 +}\StringTok{ }\DecValTok{2000}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\CommentTok{# Pdf of y from dt}
\NormalTok{pdf_y =}\StringTok{ }\KeywordTok{dt}\NormalTok{(y, df, }\DataTypeTok{ncp =} \NormalTok{shift1)}
\NormalTok{pdf_y =}\StringTok{ }\NormalTok{pdf_y/scale1}
\KeywordTok{plot}\NormalTok{(y,pdf_y, }\DataTypeTok{col =}\StringTok{"green"}\NormalTok{, }\DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\DecValTok{2000}\NormalTok{, }\DecValTok{5500}\NormalTok{), }\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{0.20}\NormalTok{),}\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{, }\DataTypeTok{main =} \StringTok{"Marginal Posterior over mean using conjugate prior"}\NormalTok{, }\DataTypeTok{xlab =} \KeywordTok{expression}\NormalTok{(mu[j]), }\DataTypeTok{ylab=} \StringTok{"p-value"}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\NormalTok{mu_y1, }\DataTypeTok{col =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{lty=}\DecValTok{5}\NormalTok{)}

\CommentTok{# For the Mild Dataset, we follow the same process}
\NormalTok{shift2 =}\StringTok{ }\NormalTok{mun_y2}
\NormalTok{scale2 =}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(alphan_y2 *}\StringTok{ }\NormalTok{nn_y2/betan_y2)}
\NormalTok{df =}\StringTok{ }\DecValTok{2}\NormalTok{*alphan_y2}

\NormalTok{y =}\StringTok{ }\KeywordTok{seq}\NormalTok{(mu_y1 -}\StringTok{ }\DecValTok{2000}\NormalTok{, mu_y1 +}\StringTok{ }\DecValTok{2000}\NormalTok{, }\DecValTok{10}\NormalTok{)}

\NormalTok{pdf_y =}\StringTok{ }\KeywordTok{dt}\NormalTok{(y, df, }\DataTypeTok{ncp =} \NormalTok{shift2)}
\NormalTok{pdf_y =}\StringTok{ }\NormalTok{pdf_y/scale2}
\KeywordTok{lines}\NormalTok{(y, pdf_y, }\DataTypeTok{col=}\StringTok{"yellow"}\NormalTok{, }\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\NormalTok{mu_y2, }\DataTypeTok{col =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{lty=}\DecValTok{5}\NormalTok{)}

\CommentTok{# For the dementia Dataset, we follow the same process again}
\NormalTok{shift3 =}\StringTok{ }\NormalTok{mun_y3}
\NormalTok{scale3 =}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(alphan_y3 *}\StringTok{ }\NormalTok{nn_y3/betan_y3)}
\NormalTok{df =}\StringTok{ }\DecValTok{2}\NormalTok{*alphan_y3}

\CommentTok{#Now, we will use the in-built t-distribution random number generator}
\NormalTok{y =}\StringTok{ }\KeywordTok{seq}\NormalTok{(mu_y1 -}\StringTok{ }\DecValTok{2000}\NormalTok{, mu_y1 +}\StringTok{ }\DecValTok{2000}\NormalTok{, }\DecValTok{10}\NormalTok{)}

\NormalTok{pdf_y =}\StringTok{ }\KeywordTok{dt}\NormalTok{(y, df, }\DataTypeTok{ncp =} \NormalTok{shift3)}
\NormalTok{pdf_y =}\StringTok{ }\NormalTok{pdf_y/scale3}
\KeywordTok{lines}\NormalTok{(y, pdf_y, }\DataTypeTok{col=}\StringTok{"black"}\NormalTok{, }\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\NormalTok{mu_y3, }\DataTypeTok{col =} \StringTok{"grey"}\NormalTok{, }\DataTypeTok{lty=}\DecValTok{5}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\StringTok{'topright'}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\StringTok{"Control"}\NormalTok{, }\StringTok{"Sample mean - control"}\NormalTok{, }\StringTok{"Mild"}\NormalTok{, }\StringTok{"Sample mean - mild"}\NormalTok{,}\StringTok{"Dementia"}\NormalTok{, }\StringTok{"Sample mean - dementia"}\NormalTok{), }\DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"green"}\NormalTok{, }\StringTok{"blue"}\NormalTok{, }\StringTok{"yellow"}\NormalTok{, }\StringTok{"red"}\NormalTok{, }\StringTok{"black"}\NormalTok{, }\StringTok{"grey"}\NormalTok{), }\DataTypeTok{lty =} \DecValTok{1}\NormalTok{:}\DecValTok{2}\NormalTok{, }\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Assignment_1_files/figure-latex/unnamed-chunk-10-1.pdf}

From the above plot it is clear that the sample of the mean values
extracted in the graph are very different from the sample means for all
the three groups.

\subsubsection{c)}\label{c-2}

Solutions: Similar to question 4c, the process and the code is as
follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n =}\StringTok{ }\DecValTok{1000000}
\CommentTok{#Calculate the parameters for the new distribution}

\CommentTok{#Datesets 1 and 2}
\NormalTok{new_mean =}\StringTok{ }\NormalTok{mun_y1 -}\StringTok{ }\NormalTok{mun_y2}
\NormalTok{var1 =}\StringTok{ }\KeywordTok{rinvgamma}\NormalTok{(n, alphan_y1, betan_y1)}
\NormalTok{var2 =}\StringTok{ }\KeywordTok{rinvgamma}\NormalTok{(n, alphan_y2, betan_y2)}
\NormalTok{new_sigma =}\StringTok{ }\KeywordTok{sqrt}\NormalTok{((var1/nn_y1)+(var2/nn_y2))}
\NormalTok{sampled_values =}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n, new_mean, new_sigma)}

\CommentTok{#Now we will calculate the mentioned probability}
\NormalTok{p_req1 =}\StringTok{ }\KeywordTok{sum}\NormalTok{(sampled_values <}\StringTok{ }\DecValTok{0}\NormalTok{)/}\KeywordTok{length}\NormalTok{(sampled_values)}
\CommentTok{#Datasets 1 and 3}
\NormalTok{new_mean =}\StringTok{ }\NormalTok{mun_y1 -}\StringTok{ }\NormalTok{mun_y3}

\NormalTok{var1 =}\StringTok{ }\KeywordTok{rinvgamma}\NormalTok{(n, alphan_y1, betan_y1)}
\NormalTok{var3 =}\StringTok{ }\KeywordTok{rinvgamma}\NormalTok{(n, alphan_y3, betan_y3)}
\NormalTok{new_sigma =}\StringTok{ }\KeywordTok{sqrt}\NormalTok{((var1/nn_y1)+}\StringTok{ }\NormalTok{(var3/nn_y3))}

\NormalTok{sampled_values =}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n, new_mean, new_sigma)}

\CommentTok{#Now we will calculate the mentioned probability}
\NormalTok{p_req2 =}\StringTok{ }\KeywordTok{sum}\NormalTok{(sampled_values <}\StringTok{ }\DecValTok{0}\NormalTok{)/}\KeywordTok{length}\NormalTok{(sampled_values)}

\CommentTok{#Datasets 2 and 3}
\NormalTok{new_mean =}\StringTok{ }\NormalTok{mun_y2 -}\StringTok{ }\NormalTok{mun_y3}
\NormalTok{var2 =}\StringTok{ }\KeywordTok{rinvgamma}\NormalTok{(n, alphan_y2, betan_y2)}
\NormalTok{var3 =}\StringTok{ }\KeywordTok{rinvgamma}\NormalTok{(n, alphan_y3, betan_y3)}
\NormalTok{new_sigma =}\StringTok{ }\KeywordTok{sqrt}\NormalTok{((var2/nn_y2)+}\StringTok{ }\NormalTok{(var3/nn_y3))}
\NormalTok{sampled_values =}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n, new_mean, new_sigma)}

\CommentTok{#Now we will calculate the mentioned probability}
\NormalTok{p_req3 =}\StringTok{ }\KeywordTok{sum}\NormalTok{(sampled_values <}\StringTok{ }\DecValTok{0}\NormalTok{)/}\KeywordTok{length}\NormalTok{(sampled_values)}
\CommentTok{#hist(sampled_values)}
\CommentTok{#print the values out}
\NormalTok{p_req1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p_req2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p_req3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.5e-05
\end{verbatim}

This question is different from ques 4 in terms of the initial
parameters. Here, the parameters are nowhere near zero in contrast to
the question 4. This change affects the results obtained. From the
results in the above three parts, we can infer that this is not a good
prior to use as the posterior means are very different from the sample
means and the same goes for the variance. Essentially, this can be a
sign of biasness of the prior which in theory should be an
un-informative prior like Jeffrey's prior.


\end{document}
