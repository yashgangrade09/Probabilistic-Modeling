---
title: "HW4"
author: "Yash Gangrade"
date: "April 8, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Question 1

```{r}
source("Markov.r", echo = FALSE, keep.source = FALSE, max.deparse.length=10000)

img1 = read_image("noisy-message.png")
img2 = read_image("noisy-yinyang.png")

#display_image(img1)
#display_image(img2)

num_col = dim(img1)[2]
num_row = dim(img1)[1]

binary_img = matrix(0, num_row, num_col)
binary_img = apply(binary_img, 1:2, function(x) rand_generator());
#display_image(binary_img)

## Question 1.1
# cat("Trying to run for different alphas and betas")
# cat("Negative Alpha and Positive High Beta")
# binary_img = gibbs_sampling_prior(binary_img, -0.5, 1, 1.5, 500, 5);
# display_image(binary_img)

# cat("Negative Alpha and Low Beta")
# binary_img = gibbs_sampling_prior(binary_img, -0.5, 0.1, 1.5, 20, 5);
# display_image(binary_img)
# 
# cat("Positive Alpha and Positive High Beta")
# binary_img = gibbs_sampling_prior(binary_img, 0.5, 1, 1.5, 20, 5);
# display_image(binary_img)
# 
# cat("Positive Alpha and Low Beta")
# binary_img = gibbs_sampling_prior(binary_img, 0.5, 0.1, 1.5, 20, 5);
# display_image(binary_img)

## Question 1.2
cat("Trying to run for different alphas and betas")
cat("Negative Alpha and Positive High Beta")
#binary_img = gibbs_sampling_prior(img1, -0.5, 1, 1.5, 20, 5);
#display_image(binary_img)

```

## Question 2 

```{r}
#source("HamiltonianMonteCarlo.r", echo = FALSE, keep.source = FALSE, max.deparse.length=10000)

## Load and Store the data so as to use for the given question
dataset = iris
dataset = subset(dataset, Species != "setosa")

# if(dataset$Species == "versicolor"){
#   dataset$Species = 1
# } else{
#   dataset$Species = 0
# }

#the upper solution was not working so found one alternative to that on StackOverflow
dataset$Species = ifelse(dataset$Species == "versicolor", 1, 0)

#print(dataset)
#dataset = as.matrix(dataset)
#print(dataset[,1])
# Seperate the dataset into features and labels

X = t(as.matrix(cbind(dataset[1:4], 1)))
Y = as.matrix(dataset[5])

#print(X[,1:30])

#print(t(rowMeans(X)))
#print(Y)

beta_i = matrix(0.0,5,1)
sigma_i = 1

# Seperate the training and training datasets 
X_Train = matrix(0.0, 5, 60)
X_Test = matrix(0.0, 5, 40)
#print(dim(X_Train))

Y_Train = matrix(0.0, 60, 1)
Y_Test = matrix(0.0, 40, 1)

# For each species, first 30 rows will be training data and the rest 20 will be testing data
X_Train[,1:30] = X[,1:30]
X_Test[,1:20] = X[,31:50]
X_Train[,31:60] = X[,51:80]
X_Test[,21:40] = X[,81:100]

Y_Train[1:30,] = Y[1:30,]
Y_Test[1:20,] = Y[31:50,]
Y_Train[31:60,] = Y[51:80,]
Y_Test[21:40,] = Y[81:100,]

X_Train = X_Train - rowMeans(X_Train)
X_Test = X_Test - rowMeans(X_Test)
# 
# print(X_Train)
# print(X_Test)

#### Main methods start from here ####

beta_norm = function(beta){
  temp = as.vector(beta)
  temp = sqrt(sum(temp * temp))
  return(temp)
}

U = function(beta = beta_i, sigma = sigma_i, data = X_Train, labels = Y_Train){
  temp_beta = beta_norm(beta)
  temp_mul = t(data) %*% beta
  temp_U = t(1 -label) %*% temp_mul + sum(log(1+exp(-temp_mul)))  + (1 / 2 * sigma^2) * temp_beta^2
  return (temp_U)
}

grad_U = function(beta = beta_i, sigma = sigma_i, data = X_Train, labels = Y_Train){
  temp_mul = t(data) %*% beta
  temp_mat = 1 - (exp(-temp_mul)/(1 + exp(-temp_mul))) - label
  temp_gradient = data %*% temp_mat + ((1/sigma^2)*beta)
  return(temp_gradient)
}



#driver function to run Hamiltonian Monte Carlo
HMC_util = function(epsilon, L, samples, burn){
  reject = 0
  beta = beta_i
  
  # starting with the burn-in samples
  for(i in 1:burn){
    beta = HMC(U(beta), grad_U(beta), epsilon, L, beta)
  }
  
  beta_l = matrix(0.0, samples, 5)
  beta_l[1,] = beta
  
  # Start with the samples/iterations
  it = samples - 1
  for(i in 1:it){
    beta_updated = HMC(U(beta), grad_U(beta), epsilon, L, beta)
    temp = beta - beta_updated
    if(beta_norm(temp) < 0.0001){
      reject = reject + 1
    }
    beta = beta_updated
    beta_l[i,] = beta_updated
  }
  accept_percent = (1 - (reject/samples))*100
  cat("Acceptance Rate is ", accept_percent, "\n")
  return(beta_l)
}

## Generating Plots

trace_beta = function(beta, ftr_num, feature){
  row = dim(beta)[1]
  plot(1:row, beta[,ftr_num], lwd = 2, col = 'blue', main="Trace Plot", xlab = "Samples", ylab = paste("Beta for ", feature, sep = ""))
}

## Code for classification
classifier = function(sample_beta, testing = X_Test){
  sample_count = dim(sample_beta)[1]
  data_points = dim(testing)[2]
  updated_labels = matrix(0, data_points, 1)
  probability_matrix = matrix(0, data_points, 1)
  
  # loop through all the values and chose one label for each beta 
  for (i in 1:sample_count) {
    beta = sample_beta[i,]
    temp_mat = t(testing) %*% beta
    temp_prob = 1 + exp(-temp_mat)
    probability_matrix = probability_matrix + (1/temp_prob)
  }
  
  probability_matrix = probability_matrix/sample_count
  updated_labels[probability_matrix > 0.5] = 1
  return(probability_matrix)
}

average_classifier = function(sample_beta, testing = X_Test){
  average_beta = colMeans(sample_beta)
  sample_count = dim(sample_beta)[1]
  data_points = dim(testing)[2]
  updated_labels = matrix(0, data_points, 1)
  probability_matrix = matrix(0, data_points, 1)
  temp_mat = t(testing) %*% beta
  temp_prob = 1 + exp(-temp_mat)
  probability_matrix = probability_matrix + (1/temp_prob)
  updated_labels[probability_matrix > 0.5] = 1
  return(updated_labels)
}

testing_classification_accuracy = function(labels_predicted, labels_true = Y_Test){
  data_count = dim(labels_predicted)[1]
  temp = matrix(0, data_count, 1)
  temp[labels_predicted == labels_true] = 1
  accuracy = (sum(temp)/data_count)*100
  cat("The accuracy for classification is ", accuracy, "\n")
}

posterior_probability = function(sample_beta, testing = X_Test){
  sample_count = dim(sample_beta)[1]
  data_points = dim(testing)[2]
  probability_matrix = matrix(0, data_points, 1)
  
  for(i in 1:sample_count){
    beta = sample_beta[1,]
    temp_mat = testing %*% beta
    probability_matrix = probability_matrix + (1/temp_prob)
  }
  probability_matrix = probability_matrix/sample_count
  return(probability_matrix)
}
```



