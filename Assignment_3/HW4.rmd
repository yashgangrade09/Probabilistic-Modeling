---
title: "HW4"
author: "Yash Gangrade"
date: "April 8, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Question 1

```{r}
source("Markov.r", echo = FALSE, keep.source = TRUE, max.deparse.length=10000)

img1 = read_image("noisy-message.png")
img2 = read_image("noisy-yinyang.png")

#display_image(img1)
#display_image(img2)

num_col = dim(img1)[2]
num_row = dim(img1)[1]

binary_img = matrix(0, num_row, num_col)
binary_img = apply(binary_img, 1:2, function(x) rand_generator());
#display_image(binary_img)

## Question 1.1
# cat("Trying to run for different alphas and betas \n")
# cat("Negative Alpha and Positive High Beta \n")
# resultant_img = gibbs_sampling_prior(binary_img, -0.3, 0.8, 1.5, 100, 5);
# display_image(resultant_img)
# #
# cat("Negative Alpha and Low Beta \n")
# resultant_img = gibbs_sampling_prior(binary_img, -0.3, 0.1, 1.5, 100, 5);
# display_image(resultant_img)
# #
# cat("Positive Alpha and Positive High Beta \n")
# resultant_img = gibbs_sampling_prior(binary_img, 0.3, 0.8, 1.5, 100, 5);
# display_image(resultant_img)
# #
# cat("Positive Alpha and Low Beta \n")
# resultant_img = gibbs_sampling_prior(binary_img, 0.3, 0.1, 1.5, 100, 5);
# display_image(resultant_img)
# 
# ## Question 1.2
# cat("Running for the Markov Noisy Image \n")
# #
# cat("Here's the noisy image i.e. noisy-message.png \n")
# display_image(img1)
# #
# cat("We used alpha = 0, beta = 2.2, sigma = 1.4 with 100 iterations and 20 burn-in samples. The clean image is shown below \n")
# resultant_img = gibbs_sampling_prior_posterior(img1, 0, 2.2, 1.4, 100, 20);
# display_image(resultant_img)

## Question 1.3
resultant_img = gen_estimated_variance(img1, 0.2, 2, 1.5, 100, 20)
display_image(resultant_img)

```



## Question 2 

```{r}
#source("HamiltonianMonteCarlo.r", echo = FALSE, keep.source = FALSE, max.deparse.length=10000)
require(animation)
## Load and Store the data so as to use for the given question
dataset = iris
dataset = subset(dataset, Species != "setosa")

# if(dataset$Species == "versicolor"){
#   dataset$Species = 1
# } else{
#   dataset$Species = 0
# }

#the upper solution was not working so found one alternative to that on StackOverflow
dataset$Species = ifelse(dataset$Species == "versicolor", 1, 0)

#print(dataset)
#dataset = as.matrix(dataset)
#print(dataset[,1])
# Seperate the dataset into features and labels

X = t(as.matrix(cbind(dataset[1:4], 1)))
Y = as.matrix(dataset[5])

#print(X[,1:30])

#print(t(rowMeans(X)))
#print(Y)

# Seperate the training and training datasets 
X_Train = matrix(0.0, 5, 60)
X_Test = matrix(0.0, 5, 40)
#print(dim(X_Train))

Y_Train = matrix(0.0, 60, 1)
Y_Test = matrix(0.0, 40, 1)

# For each species, first 30 rows will be training data and the rest 20 will be testing data
X_Train[,1:30] = X[,1:30]
X_Test[,1:20] = X[,31:50]
X_Train[,31:60] = X[,51:80]
X_Test[,21:40] = X[,81:100]

Y_Train[1:30,] = Y[1:30,]
Y_Test[1:20,] = Y[31:50,]
Y_Train[31:60,] = Y[51:80,]
Y_Test[21:40,] = Y[81:100,]

X_Train = X_Train - rowMeans(X_Train)
X_Test = X_Test - rowMeans(X_Test)
# 
# print(X_Train)
# print(X_Test)

beta_i = matrix(0.0,5,1)
sigma_i = 1

#### Main methods start from here ####

beta_norm = function(beta){
  temp = as.vector(beta)
  temp = sqrt(sum(temp * temp))
  return(temp)
}

U = function(beta = beta_i, sigma = sigma_i, data = X_Train, labels = Y_Train){
  temp_beta = beta_norm(beta)
  temp_mul = t(data) %*% beta
  temp_U = t(1 -labels) %*% temp_mul + sum(log(1+exp(-temp_mul)))  + (1 / 2 * sigma^2) * temp_beta^2
  return (temp_U)
}

grad_U = function(beta = beta_i, sigma = sigma_i, data = X_Train, labels = Y_Train){
  temp_mul = t(data) %*% beta
  temp_mat = 1 - (exp(-temp_mul)/(1 + exp(-temp_mul))) - labels
  temp_gradient = data %*% temp_mat + ((1/sigma^2)*beta)
  return(temp_gradient)
}
reject_counter = 0
## Code from Radford Neal's paper (slightly modified, adds animation)
HMC = function (U, grad_U, epsilon, L, current_q, anim = FALSE)
{
  q = current_q
  p = rnorm(length(q),0,1) # independent standard normal variates
  current_p = p
  
  qList = q
  pList = p
  
  ## Make a half step for momentum at the beginning
  p = p - epsilon * grad_U(q) / 2
  
  ## Alternate full steps for position and momentum
  for(i in 1:(L-1))
  {
    ## Make a full step for the position
    q = q + epsilon * p
    
    ## Make a full step for the momentum, except at end of trajectory
    p = p - epsilon * grad_U(q)
    
    if(anim)
    {
      H = function(x, y) { U(x) + 0.5 * y^2 }
      
      t = seq(-2, 2, 0.01)
      Hvals = outer(t, t, H)
      contour(t, t, Hvals, levels=seq(0, 3, 0.1), asp=1,
              drawlabel=FALSE, xlab="q", ylab="p")
      lines(qList, pList, lwd=2, col='red')
      ani.record()
      pList = c(pList, p)
      qList = c(qList, q)
    }
  }
  
  q = q + epsilon * p
  
  ## Make a half step for momentum at the end.
  p = p - epsilon * grad_U(q) / 2
  
  ## Negate momentum at end of trajectory to make the proposal symmetric
  p = -p
  
  ## Evaluate potential and kinetic energies at start and end of trajectory
  current_U = U(current_q)
  current_K = sum(current_p^2) / 2
  proposed_U = U(q)
  proposed_K = sum(p^2) / 2
  
  ## Accept or reject the state at end of trajectory, returning either
  ## the position at the end of the trajectory or the initial position
  if(current_U + current_K > proposed_U + proposed_K)
    return(q) # reject
  else if (runif(1) < exp(current_U-proposed_U+current_K-proposed_K))
    return (q) # accept
  else{
    reject_counter <<- reject_counter + 1
    return (current_q) # reject
  }
}

#driver function to run Hamiltonian Monte Carlo
HMC_util = function(epsilon, L, samples, burn){
  reject = 0
  beta = beta_i
  
  # starting with the burn-in samples
  for(i in 1:burn){
    beta = HMC(U, grad_U, epsilon, L, beta)
  }
  
  beta_l = matrix(0.0, samples, 5)
  beta_l[1,] = beta
  
  # Start with the samples/iterations
  it = samples - 1
  beta_updated = beta
  
  for(i in 1:it){
    beta_updated = HMC(U, grad_U, epsilon, L, beta_updated)
    temp = beta - beta_updated
    
    # if(beta_norm(temp) < 0.0001){
    #   reject = reject + 1
    # }
    beta_l[i,] = beta_updated
  }
  accept_percent = (1 - (reject_counter/samples))*100
  
  cat("Acceptance Rate is ", accept_percent, "\n")
  return(beta_l)
}

## Generating Plots

trace_beta = function(beta, ftr_num, feature){
  row = dim(beta)[1]
  plot(1:row, beta[,ftr_num], lwd = 1, type = 'l', col = 'blue', main="Trace Plot", xlab = "Samples", ylab = paste("Beta for ", feature, sep = ""))
}

hist_plot = function(beta, ftr_num, feature){
  temp = beta[,ftr_num]
  hist(temp, freq = FALSE, breaks = 75, col= "blue",main = "Histogram", xlab = "Samples", ylab = paste("Beta for ", feature, sep = ""))
}

## Code for classification
classifier = function(sample_beta, testing = X_Test){
  
  data_points = dim(testing)[2]
  sample_count = dim(sample_beta)[1]
  updated_labels = matrix(0, data_points, 1)
  probability_matrix = matrix(0, data_points, 1)
  
  # loop through all the values and chose one label for each beta by prediction
  for (i in 1:sample_count) {
    beta = sample_beta[i,]
    temp_mat = t(testing) %*% beta
    temp_prob = 1 + exp(-temp_mat)
    probability_matrix = probability_matrix + (1/temp_prob)
  }
  
  probability_matrix = probability_matrix/sample_count
  updated_labels[probability_matrix > 0.5] = 1
  return(probability_matrix)
}

average_classifier = function(sample_beta, testing = X_Test){
  
  data_points = dim(testing)[2]
  sample_count = dim(sample_beta)[1]
  average_beta = colMeans(sample_beta)
  updated_labels = matrix(0, data_points, 1)
  probability_matrix = matrix(0, data_points, 1)
  temp_mat = t(testing) %*% average_beta
  temp_prob = 1 + exp(-temp_mat)
  probability_matrix = probability_matrix + (1/temp_prob)
  updated_labels[probability_matrix > 0.5] = 1
  return(updated_labels)
}

posterior_probability = function(sample_beta, testing = X_Test){
  data_points = dim(testing)[2]
  sample_count = dim(sample_beta)[1]
  probability_matrix = matrix(0, data_points, 1)
  
  for(i in 1:sample_count){
    beta = sample_beta[i,]
    temp_mat = t(testing) %*% beta
    probability_matrix = probability_matrix + (1/(1 + exp(-temp_mat)))
  }
  probability_matrix = probability_matrix/sample_count
  return(probability_matrix)
}

testing_classification_accuracy = function(labels_predicted, labels_true = Y_Test){
  data_count = dim(labels_predicted)[1]
  temp = matrix(0, data_count, 1)
  temp[labels_predicted == labels_true] = 1
  accuracy = (sum(temp)/data_count)*100
  cat("The accuracy for classification is ", accuracy, "\n")
}

## Driver for Question 2
# 10000 samples of beta with epsilo = 0.2, L = 20, burn-in = 1000
sample_beta = HMC_util(0.2, 20, 10000, 1000)
#print(sample_beta)

## Draw Trace Plots
trace_beta(sample_beta, 1, "Sepal Length")
trace_beta(sample_beta, 2, "Sepal Width")
trace_beta(sample_beta, 3, "Petal Length")
trace_beta(sample_beta, 4, "Petal Width")
trace_beta(sample_beta, 5, "Constant")

hist_plot(sample_beta, 1, "Sepal Length")
hist_plot(sample_beta, 2, "Sepal Width")
hist_plot(sample_beta, 3, "Petal Length")
hist_plot(sample_beta, 4, "Petal Width")
hist_plot(sample_beta, 5, "Constant")

#print(sample_beta)
#print(sample_beta[,1])

## Get the prediction for y of the class labels for the testing data
prediction = classifier(sample_beta, testing = X_Train)
#print(prediction)
prediction = round(prediction)
#print(Y_Test)
testing_classification_accuracy(prediction, labels_true = Y_Train)
posterior = posterior_probability(sample_beta)
#print(posterior)

## Do for the average classification
prediction_avg = average_classifier(sample_beta)
prediction_avg = round(prediction_avg)
testing_classification_accuracy(prediction_avg)

```



